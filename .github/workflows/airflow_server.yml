name: Deploy Airflow to EC2

on:
  workflow_run:
    workflows: ["Provision Infrastructure"]  # Trigger after the first workflow completes
    types:
      - completed
    status: success

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v2

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: eu-central-1

      - name: Retrieve EC2 Public IP from S3
        run: |
          aws s3 cp s3://samkons-reddit-dataengineering/ec2_public_ip.txt ec2_public_ip.txt
          export EC2_PUBLIC_IP=$(cat ec2_public_ip.txt)
          echo "EC2_PUBLIC_IP=${EC2_PUBLIC_IP}" >> $GITHUB_ENV

      - name: Debug EC2 Public IP
        run: |
          echo "EC2_PUBLIC_IP from GITHUB_ENV: ${{ env.EC2_PUBLIC_IP }}"

      - name: Add EC2 Host Key
        run: |
          mkdir -p ~/.ssh
          ssh-keyscan -H ${{ env.EC2_PUBLIC_IP }} >> ~/.ssh/known_hosts


      - name: Decode SSH Private Key
        run: |
          echo "${{ secrets.EC2_SSH_KEY }}" > ec2_key.pem
          chmod 600 ec2_key.pem



      - name: Debug SSH Private Key
        run: |
          echo "Length of EC2_SSH_KEY: ${#EC2_SSH_KEY}"

      - name: Check ec2_key.pem
        run: |
          ls -l ec2_key.pem
          cat ec2_key.pem

      - name: Test SSH Connection
        run: |
          ssh -i ec2_key.pem ubuntu@${{ env.EC2_PUBLIC_IP }} "echo 'Connection successful'"


      - name: Debug GitHub Workspace and SCP Key Path
        run: |
          echo "GitHub Workspace: ${{ github.workspace }}"
          ls -l ${{ github.workspace }}/ec2_key.pem

      - name: Ensure correct permissions on ec2_key.pem
        run: |
          chmod 600 ${{ github.workspace }}/ec2_key.pem
          ls -l ${{ github.workspace }}/ec2_key.pem 

      - name: Upload docker-compose.yml to EC2
        uses: appleboy/scp-action@v0.1.7
        with:
          host: ${{ env.EC2_PUBLIC_IP }}
          username: ubuntu
          key: ${{ secrets.EC2_SSH_KEY }}
          source: "docker-compose.yml, config, dags,  etls, logs, pipelines, utils, glue_job, requirements.txt"
          target: /home/ubuntu/airflow/
          debug: true

      - name: Debug AIRFLOW_CORE_FERNET_KEY Secret
        run: |
          echo "Length of AIRFLOW_CORE_FERNET_KEY: ${AIRFLOW_CORE_FERNET_KEY}"
        


      - name: SSH to EC2 and set up
        run: |
          ssh -i ec2_key.pem ubuntu@${{ env.EC2_PUBLIC_IP }} << 'EOF'
          #!/bin/bash
      
          # Update and install dependencies
          sudo apt update -y
          sudo apt-get install -y python3-pip curl jq
      
          # Create required directories with appropriate permissions
          create_directories() {
            dirs=("logs" "dags" "data" "config" "plugins" "tests")
            for dir in "${dirs[@]}"; do
              mkdir -p /home/ubuntu/airflow/$dir
              sudo chown -R ubuntu:ubuntu /home/ubuntu/airflow/$dir
            done
          }

          # Create the airflow.env file with environment variables
          echo "Creating airflow.env file"
          cat <<EOT > /home/ubuntu/airflow/airflow.env
          AIRFLOW__CORE__EXECUTOR=CeleryExecutor
          AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
          AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://postgres:postgres@postgres:5432/airflow_reddit
          AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@postgres:5432/airflow_reddit
          AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
          AIRFLOW__CORE__LOGGING_LEVEL=INFO
          AIRFLOW__CORE__LOAD_EXAMPLES=False
          AIRFLOW_ADMIN_USERNAME=admin
          AIRFLOW_ADMIN_EMAIL=airflow@airflow.com
          AIRFLOW_ADMIN_PASSWORD=admin
          POSTGRES_USER=postgres
          POSTGRES_PASSWORD=postgres
          POSTGRES_DB=airflow_reddit
          EOT
      
          # Pull the Airflow Docker image
          echo "Pulling Airflow Docker image"
          docker pull samkons/samkons-custom-airflow:latest
      
          # Navigate to the Airflow directory and run Docker Compose
          echo "Navigating to the Airflow directory"
          cd /home/ubuntu/airflow
          docker-compose up -d
      
          EOF